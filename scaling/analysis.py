import json
import numpy as np
import scipy.stats
import scipy.stats.mstats
from .best_parameters import mls_best_fit, minimize_with_errors, NormalModel, LogNormalModel, PopulationModel
from .pvalue_population import pvalue_pop


MIN_VALUE = 10**-8


def cache(file_name_format):
    """
    A decorator to cache the result of the function into a file. The result
    must be a dictionary. The result storage is in json.

    The decorator argument is the file name format. The format must contain the
    same number of positional arguments as the function

    E.g. 'd_{0}_{1}.json' for a function of 2 arguments.
    """
    def cache_function(function):
        def func_wrapper(*args, **kwargs):
            file_name = file_name_format.format(*args, **kwargs)
            try:
                if 'flush' in kwargs and kwargs['flush']:
                    raise IOError
                with open(file_name, 'r', encoding='utf8') as cache_file:
                    data = json.load(cache_file)
            except IOError:
                data = function(*args, **kwargs)
                with open(file_name, 'w', encoding='utf8') as cache_file:
                    cache_file.write(json.dumps(data, ensure_ascii=False, indent=2,
                                                separators=(',', ': '),
                                                sort_keys=True))

            return data
        return func_wrapper
    return cache_function


def sort_data(x, y):
    """
    Sorts the data by increasing x while keeping the respective y.
    """
    idx = x.argsort()
    return x[idx], y[idx]


def remove_zero_y(x, y):
    """
    Removes entries of both x,y where y <= 0.
    """
    x = list(x)
    y = list(y)
    idx = []
    for i, y_i in enumerate(y[:]):
        if y_i <= 0:
            idx.append(i)
    for i in reversed(idx):
        del x[i]
        del y[i]
    return np.array(x), np.array(y)


class Analysis(object):
    """
    A general class that holds data and statistics of it.
    """
    def __init__(self, data_xy):
        self.x, self.y = data_xy

    def __str__(self):
        return self.__class__.__name__

#
# class xy(Analysis):
#     def __init__(self,dataset):
#         self.x, self.y = DATABASES[dataset]()


class MLSAnalysis(Analysis):
    """
    A MLS where we use all the data
    """
    def __init__(self, data_xy, cut=0):
        super(MLSAnalysis, self).__init__(data_xy)
        self.cut = cut
        beta, self.error, c = mls_best_fit(np.log(self.x[cut:]), np.log(self.y[cut:]))
        self.params = [beta, c]

    @property
    def beta(self):
        return self.params[0], self.error


class MLSMedianAnalysis(MLSAnalysis):
    """
    A MLS where we only use the upper half (above median) of the data.
    """
    def __init__(self, data_xy):
        x, _ = data_xy
        super(MLSMedianAnalysis, self).__init__(data_xy, int(len(x)/2.0))


class LikelihoodAnalysis(Analysis):
    """
    Contains all information of a likelihood analysis using Taylor's law.

    set the data using `x, y`; define the model using `bounds`.

    It computes `params`, `errors` and `likelihood_ratio`.
    """
    Model = NormalModel
    samples = 100

    def __init__(self, data_xy, bounds, required_successes=16):
        super(LikelihoodAnalysis, self).__init__(data_xy)
        self.bounds = bounds
        self._required_successes = required_successes
        result = self._mle_calculation(self._cache_name)

        self.params = np.array(result['params'])
        self.errors = np.array(result['errors'])
        self.minus_log_likelihood = result['-log_likelihood']

        self.p_value = self._p_value_calculation(self._cache_name)['p_value']

    @property
    def _cache_name(self):
        return '%s_%d_%d' % (self, self._required_successes, self.samples)

    def _p_value_calculation(self, cache_file):
        """
        Computes the p-value of the null hypotheses that the data was generated by the model.
        Our model assumes that:

        1. P(y|x) is normally distributed with mu(x) and sigma(x) which implies
        that z = (y - mu(x))/sigma(x) are normally distributed.
        We use the D'Agostino and Pearson K^2 test to test this assumption.

        2. z = (y - mu(x))/sigma(x) are uncorrelated. We use the spearman's correlation
        test to test this assumption.

        Since the statistics used in the tests are uncorrelated, we extend the
        K^2 test by combining the three statistics.

        p < \alpha implies that the null hypothesis is rejected.
        """

        # this ignore should not be here, but a bug in numpy forces it to be
        # See https://github.com/numpy/numpy/issues/4895
        with np.errstate(under='ignore'):
            # Combine the D'Agostino normality K^2 test (s, k) with
            # the Spearman's correlation test (sr).

            # copy of scipy.stats.mstats.normaltest
            s, _ = scipy.stats.mstats.skewtest(self.z_scores)
            k, _ = scipy.stats.mstats.kurtosistest(self.z_scores)

            # compute statistic that is normally distributed, see
            # https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient
            rs, _ = scipy.stats.spearmanr(self.x, self.z_scores)
            sr = np.sqrt((len(self.z_scores) - 3) / 1.06) * np.arctan(rs)

            # merge the 3 statistics and compute its probability
            k3 = s*s + k*k + sr*sr

            # return {'p_value': scipy.stats.chisqprob(k3, 3)}
            from scipy.stats import chi2
            # print("TODO: Check this")
            return {'p_value': chi2.sf(k3, 3)}

    #@cache('_results/mle_{1}.json')
    def _mle_calculation(self, cache_file):
        """
        This computes:
            1. the best parameters using Max. likelihood,
            2. the -log_likelihood obtained,
            3. the respective error bars using bootstrap.

        It stores the result in a json so we only have to run this once every
        code change.
        """
        model = self.Model(self.bounds)

        params, errors, minus_log_likelihood = minimize_with_errors(
            model, self.x, self.y, samples=self.samples,
            parameters={'required_successes': self._required_successes},
            disp=False)

        return {'params': list(params), 'errors': list(errors),
                '-log_likelihood': minus_log_likelihood}

    @property
    def z_scores(self):
        """
        The z-scores of the normal variable
        """
        return (self.y - self.mean)/self.std

    @property
    def beta(self):
        """
        The scaling exponent of y with x and respective error estimate
        via bootstrap.
        """
        return self.params[1], self.errors[1]

    @property
    def delta(self):
        # in this model the delta is of sigma, not of var
        return 2*self.params[3]

    @property
    def mean(self):
        """
        The mean of y.
        """
        a, b, _, _ = tuple(self.params)
        return a*np.power(self.x, b)

    @property
    def std(self):
        """
        The std of y.
        """
        _, _, c, d = tuple(self.params)
        return c*np.power(self.mean, d)

    @property
    def n_parameters(self):
        """
        Number of free parameters.
        """
        return sum([bounds[0] != bounds[1] or
                    bounds[0] is None or
                    bounds[1] is None
                    for bounds in self.bounds])

    @property
    def data_size(self):
        return len(self.x)

    @property
    def bic(self):
        # Bayesian information criterion, see e.g.
        # https://en.wikipedia.org/wiki/Bayesian_information_criterion
        return 2*self.minus_log_likelihood + self.n_parameters*np.log(self.data_size)

    def model_error_bars(self, sigmas=1):
        """
        The errors bars computed from the model.
        """
        return [sigmas*self.std, sigmas*self.std]

# The analysis
# Each analysis has the counter-part with fixed \beta=1.


class FixedDAnalysis(LikelihoodAnalysis):
    description = r'Gaussian fluctuations with \delta = 0.5'

    def __init__(self, data_xy, required_successes=16):
        super(FixedDAnalysis, self).__init__(data_xy,
                                             ([MIN_VALUE, None], [0.01, 3],
                                              [MIN_VALUE, None], [0.5, 0.5]),
                                             required_successes)


class FixedDFixedBetaAnalysis(LikelihoodAnalysis):
    description = r'Gaussian fluctuations with \beta = 1 and \delta = 0.5'

    def __init__(self, data_xy, required_successes=16):
        super(FixedDFixedBetaAnalysis, self).__init__(
            data_xy, ([MIN_VALUE, None], [1, 1], [MIN_VALUE, None], [0.5, 0.5]),
            required_successes)


class ConstrainedDAnalysis(LikelihoodAnalysis):
    description = r'Gaussian fluctuations with free \delta'

    def __init__(self, data_xy, required_successes=16):
        super(ConstrainedDAnalysis, self).__init__(data_xy,
                                                   ([MIN_VALUE, None], [0.01, 3],
                                                    [MIN_VALUE, None], [0.5, 1.0]),
                                                   required_successes)


class ConstrainedDFixedBetaAnalysis(LikelihoodAnalysis):
    description = r'Gaussian fluctuations with \beta = 1 and free \delta'

    def __init__(self, dataset, required_successes=16):
        super(ConstrainedDFixedBetaAnalysis, self).__init__(
            dataset, ([MIN_VALUE, None], [1, 1], [MIN_VALUE, None], [0.5, 1.0]),
            required_successes)


class PopulationAnalysis(LikelihoodAnalysis):
    """
    Analysis of the population model.
    """
    Model = PopulationModel
    description = r'People model with free \delta'

    def __init__(self, dataset, required_successes):
        # ([0.01, 3],) is the same as we use for beta.
        super(PopulationAnalysis, self).__init__(dataset, ([0.01, 3],), required_successes)

    # @cache('_results/pvalue_{1}.json')
    def _p_value_calculation(self, cache_file):
        return {'p_value': pvalue_pop(self.x, self.y, self.params, ([0.01, 3],))}

    @property
    def beta(self):
        return self.params[0], self.errors[0]

    @property
    def bic(self):
        # Bayesian information criterion, see e.g.
        # https://en.wikipedia.org/wiki/Bayesian_information_criterion
        sum_y = np.sum(self.y)

        ln_sum_y = sum_y*np.log(sum_y) - sum_y
        sum_ln_sum_y = np.sum(self.y*np.log(self.y) - self.y)

        return super(PopulationAnalysis, self).bic + 2*(ln_sum_y - sum_ln_sum_y)

    @property
    def delta(self):
        return 1

    @property
    def data_size(self):
        return np.sum(self.y)

    @property
    def mean(self):
        """
        The mean of this model is y*p_i, where y is the total number of tokens.
        """
        prnon = self.x**self.params[0]
        p = prnon/np.sum(prnon)
        y = np.sum(self.y)
        return y*p

    @property
    def std(self):
        prnon = self.x**self.params[0]
        p = prnon/np.sum(prnon)
        y = np.sum(self.y)
        return np.sqrt(y*p*(1-p))


class PopulationFixedGammaAnalysis(PopulationAnalysis):
    description = r'People model with \delta = 1'

    def __init__(self, dataset, required_successes):
        # ([0.01, 3],) is the same as we use for beta.
        super(PopulationAnalysis, self).__init__(dataset, ([1, 1],), required_successes)


class LogNormalAnalysis(LikelihoodAnalysis):
    description = r'Log normal fluctuations with a general \delta'
    Model = LogNormalModel

    def __init__(self, data_xy, bounds=([MIN_VALUE, None],
                                        [0.01, 3],
                                        [MIN_VALUE, None],
                                        [1, 3]),
                 required_successes=16):
        super(LikelihoodAnalysis, self).__init__(data_xy)
        self.x = np.log(self.x)
        self.y = np.log(self.y)
        self.bounds = bounds
        self._required_successes = required_successes

        result = self._mle_calculation(self._cache_name)

        self.params = np.array(result['params'])
        self.errors = np.array(result['errors'])
        self.minus_log_likelihood = result['-log_likelihood']

        self.p_value = self._p_value_calculation(self._cache_name)['p_value']

    @property
    def mean_log(self):
        """
        Median of y.
        """
        a, b, _, _ = tuple(self.params)
        return np.log(a) + b*self.x - self.var_log/2

    @property
    def var_log(self):
        """
        Variance of log_y.
        """
        _, _, c, d = tuple(self.params)
        return np.log(1 + c * np.power(self.mean, (d - 2)))

    @property
    def z_scores(self):
        """
        The z-scores of the normal variable, log_y
        """
        return (self.y - self.mean_log)/self.var_log

    @property
    def mean(self):
        a, b, _, _ = tuple(self.params)
        x = np.exp(self.x)  # self.x is log_x, see __init__.
        return a*np.power(x, b)

    @property
    def std(self):
        _, _, c, d = tuple(self.params)
        x = np.exp(self.x)  # self.x is log_x, see __init__.
        return np.sqrt(c*np.power(x, d))

    @property
    def beta(self):
        _, b, _, _ = tuple(self.params)
        _, Sb, _, _ = tuple(self.errors)
        return b, Sb

    @property
    def delta(self):
        return self.params[3]

    def model_error_bars(self, sigmas=1):
        return [self.mean - np.exp(self.mean_log - sigmas*np.sqrt(self.var_log)),
                np.exp(self.mean_log + sigmas*np.sqrt(self.var_log)) - self.mean]


class LogNormalFixedBetaAnalysis(LogNormalAnalysis):
    description = r'Log normal fluctuations with \beta = 1'

    def __init__(self, dataset, required_successes=16):
        super(LogNormalFixedBetaAnalysis, self).__init__(
            dataset, ([MIN_VALUE, None], [1, 1], [MIN_VALUE, None], [1, 3]),
            required_successes)


class LogNormalFixedDAnalysis(LogNormalAnalysis):
    description = r'Log normal fluctuations with \delta = 1'

    def __init__(self, dataset, required_successes=16):
        super(LogNormalFixedDAnalysis, self).__init__(
            dataset, ([MIN_VALUE, None], [0.01, 3], [MIN_VALUE, None], [2, 2]),
            required_successes)


class LogNormalFixedDFixedBetaAnalysis(LogNormalAnalysis):
    description = r'Log normal fluctuations with \beta = \delta = 1'

    def __init__(self, dataset, required_successes=16):
        super(LogNormalFixedDFixedBetaAnalysis, self).__init__(
            dataset, ([MIN_VALUE, None], [1, 1], [MIN_VALUE, None], [2, 2]),
            required_successes)


MODELS = dict((x.__name__, x) for x in [
    ConstrainedDAnalysis, ConstrainedDFixedBetaAnalysis,
    FixedDAnalysis, FixedDFixedBetaAnalysis,
    LogNormalAnalysis, LogNormalFixedBetaAnalysis,
    LogNormalFixedDAnalysis, LogNormalFixedDFixedBetaAnalysis,
    PopulationAnalysis, PopulationFixedGammaAnalysis
])


MODEL_TO_MODEL = {'FixedDAnalysis': 'FixedDFixedBetaAnalysis',
                  'ConstrainedDAnalysis': 'ConstrainedDFixedBetaAnalysis',
                  'LogNormalAnalysis': 'LogNormalFixedBetaAnalysis',
                  'LogNormalFixedDAnalysis': 'LogNormalFixedDFixedBetaAnalysis',
                  'PopulationAnalysis': 'PopulationFixedGammaAnalysis'}


FREE_MODEL_TO_FIXED_MODEL = {
    'ConstrainedDAnalysis': 'FixedDAnalysis',
    'LogNormalAnalysis': 'LogNormalFixedDAnalysis',
}

FIXED_MODEL_TO_FREE_MODEL = {
    'FixedDAnalysis': 'ConstrainedDAnalysis',
    'LogNormalFixedDAnalysis': 'LogNormalAnalysis',
}
